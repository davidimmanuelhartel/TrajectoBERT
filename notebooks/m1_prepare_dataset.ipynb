{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import typing\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset\n",
    "from torchtext.vocab import vocab\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBBertDataset(Dataset):\n",
    "    # Define Special tokens as attributes of class\n",
    "    CLS = '[CLS]'\n",
    "    PAD = '[PAD]'\n",
    "    SEP = '[SEP]'\n",
    "    MASK = '[MASK]'\n",
    "    UNK = '[UNK]'\n",
    "\n",
    "    MASK_PERCENTAGE = 0.15  # How much words to mask\n",
    "\n",
    "    MASKED_INDICES_COLUMN = 'masked_indices'\n",
    "    TARGET_COLUMN = 'indices'\n",
    "    NSP_TARGET_COLUMN = 'is_next'\n",
    "    TOKEN_MASK_COLUMN = 'token_mask'\n",
    "\n",
    "    OPTIMAL_LENGTH_PERCENTILE = 70\n",
    "\n",
    "    def __init__(self, path, ds_from=None, ds_to=None, should_include_text=False):\n",
    "        self.ds: pd.Series = pd.read_csv(path)['review']\n",
    "\n",
    "        if ds_from is not None or ds_to is not None:\n",
    "            self.ds = self.ds[ds_from:ds_to]\n",
    "\n",
    "        self.tokenizer = get_tokenizer('basic_english')\n",
    "        self.counter = Counter()\n",
    "        self.vocab = None\n",
    "\n",
    "        self.optimal_sentence_length = None\n",
    "        self.should_include_text = should_include_text\n",
    "\n",
    "        if should_include_text:\n",
    "            self.columns = ['masked_sentence', self.MASKED_INDICES_COLUMN, 'sentence', self.TARGET_COLUMN,\n",
    "                            self.TOKEN_MASK_COLUMN,\n",
    "                            self.NSP_TARGET_COLUMN]\n",
    "        else:\n",
    "            self.columns = [self.MASKED_INDICES_COLUMN, self.TARGET_COLUMN, self.TOKEN_MASK_COLUMN,\n",
    "                            self.NSP_TARGET_COLUMN]\n",
    "        self.df = self.prepare_dataset()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "            item = self.df.iloc[idx]\n",
    "\n",
    "            inp = torch.Tensor(item[self.MASKED_INDICES_COLUMN]).long()\n",
    "            token_mask = torch.Tensor(item[self.TOKEN_MASK_COLUMN]).bool()\n",
    "\n",
    "            mask_target = torch.Tensor(item[self.TARGET_COLUMN]).long()\n",
    "            mask_target = mask_target.masked_fill_(token_mask, 0)\n",
    "\n",
    "            attention_mask = (inp == self.vocab[self.PAD]).unsqueeze(0)\n",
    "\n",
    "            if item[self.NSP_TARGET_COLUMN] == 0:\n",
    "                t = [1, 0]\n",
    "            else:\n",
    "                t = [0, 1]\n",
    "\n",
    "            nsp_target = torch.Tensor(t)\n",
    "\n",
    "            return (\n",
    "                inp.to(device),\n",
    "                attention_mask.to(device),\n",
    "                token_mask.to(device),\n",
    "                mask_target.to(device),\n",
    "                nsp_target.to(device)\n",
    "        )\n",
    "\n",
    "    def prepare_dataset(self) -> pd.DataFrame:\n",
    "        sentences = []\n",
    "        nsp = []\n",
    "        sentence_lens = []\n",
    "\n",
    "        # Split dataset on sentences\n",
    "        for review in self.ds:\n",
    "            review_sentences = review.split('. ')\n",
    "            sentences += review_sentences\n",
    "            self._update_length(review_sentences, sentence_lens)\n",
    "        self.optimal_sentence_length = self._find_optimal_sentence_length(sentence_lens)\n",
    "\n",
    "        print(\"Create vocabulary\")\n",
    "        for sentence in tqdm(sentences):\n",
    "            s = self.tokenizer(sentence)\n",
    "            self.counter.update(s)\n",
    "\n",
    "        self._fill_vocab()\n",
    "\n",
    "        print(\"Preprocessing dataset\")\n",
    "        for review in tqdm(self.ds):\n",
    "            review_sentences = review.split('. ')\n",
    "            if len(review_sentences) > 1:\n",
    "                for i in range(len(review_sentences) - 1):\n",
    "                    # True NSP item\n",
    "                    first, second = self.tokenizer(review_sentences[i]), self.tokenizer(review_sentences[i + 1])\n",
    "                    nsp.append(self._create_item(first, second, 1))\n",
    "\n",
    "                    # False NSP item\n",
    "                    first, second = self._select_false_nsp_sentences(sentences)\n",
    "                    first, second = self.tokenizer(first), self.tokenizer(second)\n",
    "                    nsp.append(self._create_item(first, second, 0))\n",
    "        df = pd.DataFrame(nsp, columns=self.columns)\n",
    "        return df\n",
    "\n",
    "    def _update_length(self, sentences: typing.List[str], lengths: typing.List[int]):\n",
    "        for v in sentences:\n",
    "            l = len(v.split())\n",
    "            lengths.append(l)\n",
    "        return lengths\n",
    "\n",
    "    def _find_optimal_sentence_length(self, lengths: typing.List[int]):\n",
    "        arr = np.array(lengths)\n",
    "        return int(np.percentile(arr, self.OPTIMAL_LENGTH_PERCENTILE))\n",
    "\n",
    "    def _fill_vocab(self):\n",
    "        # specials= argument is only in 0.12.0 version\n",
    "        # specials=[self.CLS, self.PAD, self.MASK, self.SEP, self.UNK]\n",
    "        self.vocab = vocab(self.counter, min_freq=2)\n",
    "\n",
    "        # 0.11.0 uses this approach to insert specials\n",
    "        self.vocab.insert_token(self.CLS, 0)\n",
    "        self.vocab.insert_token(self.PAD, 1)\n",
    "        self.vocab.insert_token(self.MASK, 2)\n",
    "        self.vocab.insert_token(self.SEP, 3)\n",
    "        self.vocab.insert_token(self.UNK, 4)\n",
    "        self.vocab.set_default_index(4)\n",
    "\n",
    "    def _create_item(self, first: typing.List[str], second: typing.List[str], target: int = 1):\n",
    "        # Create masked sentence item\n",
    "        updated_first, first_mask = self._preprocess_sentence(first.copy())\n",
    "        updated_second, second_mask = self._preprocess_sentence(second.copy())\n",
    "\n",
    "        nsp_sentence = updated_first + [self.SEP] + updated_second\n",
    "        nsp_indices = self.vocab.lookup_indices(nsp_sentence)\n",
    "        inverse_token_mask = first_mask + [True] + second_mask\n",
    "\n",
    "        # Create sentence item without masking random words\n",
    "        first, _ = self._preprocess_sentence(first.copy(), should_mask=False)\n",
    "        second, _ = self._preprocess_sentence(second.copy(), should_mask=False)\n",
    "        original_nsp_sentence = first + [self.SEP] + second\n",
    "        original_nsp_indices = self.vocab.lookup_indices(original_nsp_sentence)\n",
    "\n",
    "        if self.should_include_text:\n",
    "            return (\n",
    "                nsp_sentence,\n",
    "                nsp_indices,\n",
    "                original_nsp_sentence,\n",
    "                original_nsp_indices,\n",
    "                inverse_token_mask,\n",
    "                target\n",
    "            )\n",
    "        else:\n",
    "            return (\n",
    "                nsp_indices,\n",
    "                original_nsp_indices,\n",
    "                inverse_token_mask,\n",
    "                target\n",
    "            )\n",
    "\n",
    "    def _select_false_nsp_sentences(self, sentences: typing.List[str]):\n",
    "        \"\"\"Select sentences to create false NSP item\n",
    "\n",
    "        Args:\n",
    "            sentences: list of all sentences\n",
    "\n",
    "        Returns:\n",
    "            tuple of two sentences. The second one NOT the next sentence\n",
    "        \"\"\"\n",
    "        sentences_len = len(sentences)\n",
    "        sentence_index = random.randint(0, sentences_len - 1)\n",
    "        next_sentence_index = random.randint(0, sentences_len - 1)\n",
    "\n",
    "        # To be sure that it's not real next sentence\n",
    "        while next_sentence_index == sentence_index + 1:\n",
    "            next_sentence_index = random.randint(0, sentences_len - 1)\n",
    "\n",
    "        return sentences[sentence_index], sentences[next_sentence_index]\n",
    "\n",
    "    def _preprocess_sentence(self, sentence: typing.List[str], should_mask: bool = True):\n",
    "        inverse_token_mask = None\n",
    "        if should_mask:\n",
    "            sentence, inverse_token_mask = self._mask_sentence(sentence)\n",
    "        sentence, inverse_token_mask = self._pad_sentence([self.CLS] + sentence, [True] + inverse_token_mask)\n",
    "\n",
    "        return sentence, inverse_token_mask\n",
    "\n",
    "    def _mask_sentence(self, sentence: typing.List[str]):\n",
    "        \"\"\"Replace MASK_PERCENTAGE (15%) of words with special [MASK] symbol\n",
    "        or with random word from vocabulary\n",
    "\n",
    "        Args:\n",
    "            sentence: sentence to process\n",
    "\n",
    "        Returns:\n",
    "            tuple of processed sentence and inverse token mask\n",
    "        \"\"\"\n",
    "        len_s = len(sentence)\n",
    "        inverse_token_mask = [True for _ in range(max(len_s, self.optimal_sentence_length))]\n",
    "\n",
    "        mask_amount = round(len_s * self.MASK_PERCENTAGE)\n",
    "        for _ in range(mask_amount):\n",
    "            i = random.randint(0, len_s - 1)\n",
    "\n",
    "            if random.random() < 0.8:\n",
    "                sentence[i] = self.MASK\n",
    "            else:\n",
    "                # All is below 5 is special token\n",
    "                # see self._insert_specials method\n",
    "                j = random.randint(5, len(self.vocab) - 1)\n",
    "                sentence[i] = self.vocab.lookup_token(j)\n",
    "            inverse_token_mask[i] = False\n",
    "        return sentence, inverse_token_mask\n",
    "\n",
    "    def _pad_sentence(self, sentence: typing.List[str], inverse_token_mask: typing.List[bool] = None):\n",
    "        len_s = len(sentence)\n",
    "\n",
    "        if len_s >= self.optimal_sentence_length:\n",
    "            s = sentence[:self.optimal_sentence_length]\n",
    "        else:\n",
    "            s = sentence + [self.PAD] * (self.optimal_sentence_length - len_s)\n",
    "\n",
    "        # inverse token mask should be padded as well\n",
    "        if inverse_token_mask:\n",
    "            len_m = len(inverse_token_mask)\n",
    "            if len_m >= self.optimal_sentence_length:\n",
    "                inverse_token_mask = inverse_token_mask[:self.optimal_sentence_length]\n",
    "            else:\n",
    "                inverse_token_mask = inverse_token_mask + [True] * (self.optimal_sentence_length - len_m)\n",
    "        return s, inverse_token_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
